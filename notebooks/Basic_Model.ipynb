{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f5f916",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-02 01:58:22.369177: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-02 01:58:22.846771: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/alirahman/miniconda3/envs/tf/lib/python3.9/site-packages/cv2/../../lib64::/home/alirahman/miniconda3/envs/tf/lib/:/home/alirahman/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-03-02 01:58:22.846819: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/alirahman/miniconda3/envs/tf/lib/python3.9/site-packages/cv2/../../lib64::/home/alirahman/miniconda3/envs/tf/lib/:/home/alirahman/.mujoco/mujoco210/bin:/usr/lib/nvidia\n",
      "2023-03-02 01:58:22.846825: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c3c433f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alirahman/miniconda3/envs/tf/lib/python3.9/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/alirahman/miniconda3/envs/tf/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:45: UserWarning: \u001b[33mWARN: A Box observation space maximum and minimum values are equal.\u001b[0m\n",
      "  logger.warn(\"A Box observation space maximum and minimum values are equal.\")\n",
      "/home/alirahman/miniconda3/envs/tf/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:174: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed a `seed` instead of using `Env.seed` for resetting the environment random number generator.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/alirahman/miniconda3/envs/tf/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:187: UserWarning: \u001b[33mWARN: Future gym versions will require that `Env.reset` can be passed `options` to allow the environment initialisation to be passed additional information.\u001b[0m\n",
      "  logger.warn(\n",
      "/home/alirahman/miniconda3/envs/tf/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:195: UserWarning: \u001b[33mWARN: The result returned by `env.reset()` was not a tuple of the form `(obs, info)`, where `obs` is a observation and `info` is a dictionary containing additional information. Actual type: `<class 'dict'>`\u001b[0m\n",
      "  logger.warn(\n",
      "/home/alirahman/miniconda3/envs/tf/lib/python3.9/site-packages/gym/utils/passive_env_checker.py:272: UserWarning: \u001b[33mWARN: No render modes was declared in the environment (env.metadata['render_modes'] is None or not defined), you may have trouble when calling `.render()`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=400x400>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"sawyer:PickPlace-v0\", cam_id=0, action_scale=2/100)\n",
    "env.reset()\n",
    "env.hide_mocap()\n",
    "env.reset()\n",
    "#p = env.action_space.sample()\n",
    "env.put_obj_in_hand()\n",
    "img, depth = env.render('rgbd', width=400, height=400)\n",
    "Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd2c54c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)\n",
      "Box(-1.0, 1.0, (4,), float32)\n",
      "2\n",
      "Dict('desired_goal': Box([-0.1  0.4  0.1 -0.1  0.4  0.1], [0.1  0.6  0.22 0.1  0.6  0.22], (6,), float32), 'achieved_goal': Box([-0.1  0.4  0.1 -0.1  0.4  0.1], [0.1  0.6  0.22 0.1  0.6  0.22], (6,), float32), 'observation': Box([-0.1   0.4   0.1  -0.1   0.4   0.1   0.    0.5   0.02], [0.1  0.6  0.22 0.1  0.6  0.22 0.   0.5  0.02], (9,), float32), 'state_observation': Box([-0.1   0.4   0.1  -0.1   0.4   0.1   0.    0.5   0.02], [0.1  0.6  0.22 0.1  0.6  0.22 0.   0.5  0.02], (9,), float32), 'state_desired_goal': Box([-0.1  0.4  0.1 -0.1  0.4  0.1], [0.1  0.6  0.22 0.1  0.6  0.22], (6,), float32), 'state_achieved_goal': Box([-0.1  0.4  0.1 -0.1  0.4  0.1], [0.1  0.6  0.22 0.1  0.6  0.22], (6,), float32), 'state_delta': Box([-0.1  0.4  0.1 -0.1  0.4  0.1], [0.1  0.6  0.22 0.1  0.6  0.22], (6,), float32), 'state_touch_distance': Box([-0.1  0.4  0.1], [0.1  0.6  0.22], (3,), float32), 'state_gripper': Box(-0.03, 0.03, (4,), float64))\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "from collections import deque\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import InputLayer, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from baselines import bench\n",
    "from baselines.common.vec_env.dummy_vec_env import DummyVecEnv\n",
    "#from baselines.common import plot_util as pu\n",
    "from baselines.common.vec_env.vec_normalize import VecNormalize as VecNormalize_\n",
    "\n",
    "\n",
    "class DiagGaussian():\n",
    "    def __init__(self, mean, logstd):\n",
    "        self.set_param(mean, logstd)\n",
    "\n",
    "    def set_param(self, mean, logstd): # Used so that we don't need to make a new DiagGaussian object every time mu and log_sigma changes\n",
    "        self.mean = mean\n",
    "        self.logstd = logstd\n",
    "        self.std = tf.exp(logstd)\n",
    "\n",
    "    def mode(self):\n",
    "        return self.mean\n",
    "\n",
    "    def logp(self, x):\n",
    "        neg_logp = 0.5 * tf.reduce_sum(tf.square((x - self.mean) / self.std), axis=-1) \\\n",
    "                 + 0.5 * np.log(2.0 * np.pi) * tf.cast(tf.shape(x)[-1], dtype=tf.float32) \\\n",
    "                 + tf.reduce_sum(self.logstd, axis=-1)\n",
    "        return -tf.expand_dims(neg_logp, axis=-1) # Expand dims to get correct shape\n",
    "\n",
    "    def entropy(self):\n",
    "        return tf.expand_dims(tf.reduce_sum(self.logstd + .5 * np.log(2.0 * np.pi * np.e), axis=-1), axis=-1) # Expand dims to get correct shape\n",
    "\n",
    "    def sample(self):\n",
    "        return self.mean + self.std * tf.random.normal(tf.shape(self.mean))\n",
    "    \n",
    "\n",
    "class VecNormalize(VecNormalize_): \n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(VecNormalize, self).__init__(*args, **kwargs)\n",
    "        self.training = True\n",
    "\n",
    "    def _obfilt(self, obs, update=True):\n",
    "        if self.ob_rms:\n",
    "            if self.training and update:\n",
    "                self.ob_rms.update(obs)\n",
    "            obs = np.clip((obs - self.ob_rms.mean) /\n",
    "                          np.sqrt(self.ob_rms.var + self.epsilon),\n",
    "                          -self.clipob, self.clipob)\n",
    "            return obs\n",
    "        else:\n",
    "            return obs\n",
    "    \n",
    "    def step_wait(self):\n",
    "        obs, rews, news, infos = self.venv.step_wait()\n",
    "        self.ret = self.ret * self.gamma + rews\n",
    "        obs = self._obfilt(obs)\n",
    "        if self.ret_rms:\n",
    "            self.ret_rms.update(self.ret)\n",
    "            rews = np.clip(rews / np.sqrt(self.ret_rms.var + self.epsilon), -self.cliprew, self.cliprew)\n",
    "        self.ret[news] = 0.\n",
    "        return obs[-1], rews[-1], news[-1], infos[-1] # Indexing at -1 since we will only ever have 1 env in wrapper\n",
    "    \n",
    "    def reset(self):\n",
    "        self.ret = np.zeros(self.num_envs)\n",
    "        obs = self.venv.reset()\n",
    "        return self._obfilt(obs)[-1] # Indexing at -1 since we will only ever have 1 env in wrapper\n",
    "\n",
    "    def train(self):\n",
    "        self.training = True\n",
    "\n",
    "    def eval(self):\n",
    "        self.training = False # Prevents ob_rms from updating during testing\n",
    "\n",
    "# Checks whether done was caused by timit limits or not\n",
    "class TimeLimitMask(gym.Wrapper):\n",
    "    def step(self, action):\n",
    "        obs, rew, done, info = self.env.step(action)\n",
    "        if done and self.env._max_episode_steps == self.env._elapsed_steps:\n",
    "            info['bad_transition'] = True\n",
    "\n",
    "        return obs, rew, done, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "    \n",
    "    \n",
    "class Model_Actor_Critic(Model):\n",
    "    def __init__(self, state_space_dim, action_space_dim,\n",
    "                 actor_hidden_units, critic_hidden_units):\n",
    "        super(Model_Actor_Critic, self).__init__()\n",
    "\n",
    "        self.input_layer = InputLayer(input_shape=(state_space_dim,))\n",
    "        \n",
    "        # Actor layers\n",
    "        self.actor_hidden_layers = []\n",
    "        for units in actor_hidden_units:\n",
    "            self.actor_hidden_layers.append(Dense(units=units, activation='relu', kernel_initializer='GlorotNormal'))\n",
    "        self.mu = Dense(units=action_space_dim, activation='tanh', kernel_initializer='GlorotNormal')\n",
    "        self.log_sigma = tf.Variable(initial_value=np.zeros(shape=(1, action_space_dim)), dtype=tf.float32) \n",
    "        self.trainable_weights.append(self.log_sigma) # Add log_sigma to list of trainable variables for model\n",
    "\n",
    "        # Critic layers\n",
    "        self.critic_hidden_layers = []\n",
    "        for units in critic_hidden_units:\n",
    "            self.critic_hidden_layers.append(Dense(units=units, activation='relu', kernel_initializer='GlorotNormal'))\n",
    "        self.values = Dense(units=1, activation=None, kernel_initializer='GlorotNormal')\n",
    "\n",
    "        self.dist = DiagGaussian(self.mu, self.log_sigma)\n",
    "\n",
    "    # @tf.function decoration runs function in graph mode - signficantly faster than eager execution in tf 2.0\n",
    "    @tf.function\n",
    "    def __call__(self, states):\n",
    "        states = tf.cast(states, tf.float32) # Single precision required, explict casting in __call__\n",
    "        x = y = self.input_layer(states)\n",
    "\n",
    "        for layer in self.actor_hidden_layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        for layer in self.critic_hidden_layers:\n",
    "            y = layer(y)\n",
    "            \n",
    "        return self.mu(x), self.values(y)\n",
    "\n",
    "    def act(self, state):\n",
    "        mu, value = self(state[np.newaxis, :]) # Add new axis to make tensor the correct shape (batch size, state_dim)\n",
    "        self.dist.set_param(mu, self.log_sigma)\n",
    "\n",
    "        action = self.dist.sample()\n",
    "        action_logp = self.dist.logp(action)\n",
    "        \n",
    "        return value[0].numpy(), action[0].numpy(), action_logp[0].numpy()\n",
    "    \n",
    "    def evaluate_actions(self, states, actions):\n",
    "        mu, values = self(states)\n",
    "        self.dist.set_param(mu, self.log_sigma)\n",
    "\n",
    "        return values, self.dist.logp(actions), self.dist.entropy()\n",
    "    \n",
    "    def get_value(self, state):\n",
    "        _, value = self(state[np.newaxis, :]) # Add new axis to make tensor the correct shape (batch size, state_dim)\n",
    "\n",
    "        return value[0].numpy()\n",
    "\n",
    "    \n",
    "class PPO_Agent():\n",
    "    def __init__(self, params, env_name, model_dir, log_dir, plot_dir, seed=0):\n",
    "        self.params = params\n",
    "\n",
    "        self.env_name = env_name\n",
    "\n",
    "        self.seed = seed\n",
    "\n",
    "        self.model_dir = model_dir\n",
    "        self.plot_dir = plot_dir\n",
    "        self.log_dir = log_dir\n",
    "    \n",
    "    def construct_agent(self):\n",
    "        self.env = gym.make(self.env_name) # Note: Do not unwrap, only need general attributes of env\n",
    "\n",
    "        # Set seed for deterministic results\n",
    "        np.random.seed(self.seed)\n",
    "        tf.random.set_seed(self.seed)\n",
    "        # self.env.seed(self.seed)\n",
    "\n",
    "        # TimeLimit wrapper useful for Mujoco environments\n",
    "        self.env = TimeLimitMask(self.env)\n",
    "\n",
    "        # Models - training step function, networks, and optimizer must be redefined/reconstructed after switching environments\n",
    "        self.actor_critic = Model_Actor_Critic(self.env.observation_space.shape[0], \n",
    "                                               self.env.action_space.shape[0], \n",
    "                                               self.params['ACTOR_HIDDEN_UNITS'],\n",
    "                                               self.params['CRITIC_HIDDEN_UNITS'])\n",
    "        self.train_model = self.get_train_model_function()\n",
    "        self.optimizer = Adam(learning_rate=self.params['LEARNING_RATE'], epsilon=self.params['OPTIMIZER_EPSILON'])\n",
    "\n",
    "    def set_env(self, env_name, **kwargs): # 'seed' and 'params' are optional arguments\n",
    "        self.env_name = env_name\n",
    "        self.params = kwargs.get('params', self.params)\n",
    "        self.seed = kwargs.get('seed', self.seed)\n",
    "\n",
    "    def calculate_returns(self, rewards, masks, bad_masks, values):\n",
    "        # Use generalized advantage estimator (balance of bias & variance with lambda-return compared to TD and MC)\n",
    "        # If lambda = 1, then becomes MC. If lambda is 0, then becomes TD\n",
    "        returns = []\n",
    "        gae = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            delta = rewards[i] + self.params['GAMMA'] * values[i + 1] * masks[i] - values[i]\n",
    "            gae = delta + self.params['GAMMA'] * self.params['LAMBDA'] * masks[i] * gae\n",
    "            gae = gae * bad_masks[i] # If not a true transition due to environment reset, set gae to 0\n",
    "            returns.insert(0, gae) # Keep inserting at beginning because we're traversing in reverse order\n",
    "        returns += values[:-1]                                                                         \n",
    "        return returns\n",
    "\n",
    "    # Returns a new function whenever environments are switched because tf.function creates a graph specific to the function\n",
    "    def get_train_model_function(self):\n",
    "        # @tf.function decoration runs function in graph mode - signficantly faster than eager execution in tf 2.0\n",
    "        @tf.function\n",
    "        def train_model(states, actions, returns, old_values, old_logps):\n",
    "            advantages = returns - old_values\n",
    "            mean, var = tf.nn.moments(advantages, [0], keepdims=True) # Standardize advantages in each MINIBATCH\n",
    "            advantages = (advantages - mean) / (tf.sqrt(var) + 1e-8)\n",
    "\n",
    "            with tf.GradientTape() as tape:\n",
    "                values, logps, dist_entropy = self.actor_critic.evaluate_actions(states, actions)\n",
    "\n",
    "                ratio = tf.exp(logps - old_logps)\n",
    "                clipped_ratio = tf.clip_by_value(ratio, 1 - self.params['CLIP_PARAM'], 1 + self.params['CLIP_PARAM'])\n",
    "\n",
    "                actor_loss = tf.reduce_mean(tf.minimum(ratio * advantages, clipped_ratio * advantages))\n",
    "                critic_loss = tf.reduce_mean(0.5 * tf.square(returns - values))\n",
    "\n",
    "                loss = -actor_loss + (self.params['VALUE_FUNCTION_COEF'] * critic_loss) - (self.params['ENTROPY_COEF'] * dist_entropy)\n",
    "\n",
    "            gradients = tape.gradient(loss, self.actor_critic.trainable_weights)\n",
    "            self.optimizer.apply_gradients(zip(gradients, self.actor_critic.trainable_weights))\n",
    "        return train_model\n",
    "\n",
    "    def train(self):\n",
    "        # Construct agent\n",
    "        self.construct_agent()\n",
    "\n",
    "        # Create log directory\n",
    "        os.makedirs(os.path.join(self.log_dir, self.env_name.split('-')[0]), exist_ok=True)\n",
    "\n",
    "        # Construct testing environment - bench.Monitor used for logging episodes, VecNormalize for normalizing states & rewards\n",
    "        self.env = bench.Monitor(self.env, os.path.join(self.log_dir, self.env_name.split('-')[0], '0')) # FILE PATH MUST HAVE A DIGIT AT END\n",
    "        self.env = DummyVecEnv([lambda: self.env]) # Needed to use the VecNormalize wrapper\n",
    "        self.env = VecNormalize(self.env, \n",
    "                                ob=True, ret=True, \n",
    "                                gamma=self.params['GAMMA']) # Normalize states AND rewards for training\n",
    "\n",
    "        episode_reward_summary = deque(maxlen=10)\n",
    "\n",
    "        state = self.env.reset() # Reset once at beginning, all subsequent resets handled by monitor\n",
    "\n",
    "        NUM_TRAIN_UPDATES = int(self.params['NUM_ENV_TIMESTEPS']) // self.params['NUM_TIMESTEPS_PER_UPDATE']\n",
    "        MINIBATCH_SIZE = int(self.params['NUM_TIMESTEPS_PER_UPDATE']) // self.params['NUM_MINIBATCHES']\n",
    "\n",
    "        for update in range(1, NUM_TRAIN_UPDATES + 1):\n",
    "            start_time = time.time()\n",
    "\n",
    "            total_update_reward = 0\n",
    "\n",
    "            states = []\n",
    "            actions = []\n",
    "            rewards = []\n",
    "            masks = []\n",
    "            bad_masks = []\n",
    "            values = []\n",
    "            old_logps = []\n",
    "            \n",
    "            \"\"\"Perform Rollout\"\"\"\n",
    "            for update_step in range(1, self.params['NUM_TIMESTEPS_PER_UPDATE'] + 1):\n",
    "                value, action, action_logp = self.actor_critic.act(state)\n",
    "                \n",
    "                next_state, reward, done, info = self.env.step(action)\n",
    "\n",
    "                if 'episode' in info.keys():\n",
    "                    episode_reward_summary.append(info['episode']['r']) # Logged by bench.Monitor\n",
    "                \n",
    "                states.append(state)\n",
    "                actions.append(action)\n",
    "                rewards.append(reward)\n",
    "                old_logps.append(action_logp)\n",
    "                values.append(value)\n",
    "                masks.append((0.0 if done else 1.0))\n",
    "                bad_masks.append((0.0 if 'bad_transition' in info.keys() else 1.0)) # Occurs when a done is the result\n",
    "                                                                                    # of exceeding the environment step limit\n",
    "                state = next_state\n",
    "            \n",
    "            values.append(self.actor_critic.get_value(next_state))\n",
    "\n",
    "            \"\"\"Update Networks\"\"\"\n",
    "            states = np.asarray(states, dtype=np.float32)\n",
    "            actions = np.asarray(actions, dtype=np.float32)\n",
    "            rewards = np.asarray(rewards, dtype=np.float32)\n",
    "            masks = np.asarray(masks, dtype=np.float32)\n",
    "            bad_masks = np.asarray(bad_masks, dtype=np.float32)\n",
    "            values = np.asarray(values, dtype=np.float32)\n",
    "            old_logps = np.asarray(old_logps, dtype=np.float32)\n",
    "\n",
    "            # Caclulate returns\n",
    "            returns = self.calculate_returns(rewards, masks, bad_masks, values)\n",
    "\n",
    "            # Create minibatches and train model\n",
    "            inds = np.arange(self.params['NUM_TIMESTEPS_PER_UPDATE'])\n",
    "            for _ in range(self.params['NUM_EPOCHS']):\n",
    "                np.random.shuffle(inds) # IMPORTANT: shuffling data indices for training stability. Since this problem is a sequential task,\n",
    "                                        # consecutive samples are very similar to each other in unshuffled data. Our minibatch sizes are already\n",
    "                                        # fairly small, so not shuffling could lead us to have conflicting gradients for each minibatch.\n",
    "                for start in range(0, self.params['NUM_TIMESTEPS_PER_UPDATE'], MINIBATCH_SIZE):\n",
    "                    end = start + MINIBATCH_SIZE\n",
    "                    mb_inds = inds[start:end]\n",
    "                    mb = (arr[mb_inds] for arr in (states, actions, returns, values, old_logps))\n",
    "\n",
    "                    self.train_model(*mb)\n",
    "            \n",
    "            end_time = time.time()\n",
    "            elapsed_time = end_time - start_time\n",
    "\n",
    "            \"\"\" Print Summary \"\"\"\n",
    "            print(\"Update {}/{}, Timesteps completed {}\\nLast {} episodes: mean/median reward {:.1f}/{:.1f}, min/max reward {:.1f}/{:.1f}, elapsed update time {:.3f} seconds\\n\"\n",
    "                    .format(update, NUM_TRAIN_UPDATES, self.params['NUM_TIMESTEPS_PER_UPDATE'] * update, len(episode_reward_summary),\n",
    "                    np.mean(episode_reward_summary), np.median(episode_reward_summary), np.min(episode_reward_summary),\n",
    "                    np.max(episode_reward_summary), elapsed_time))\n",
    "\n",
    "            if (update % self.params['SAVE_INTERVAL'] == 0 or update == NUM_TRAIN_UPDATES):\n",
    "                # Save model weights\n",
    "                model_path = os.path.join(self.model_dir, self.env_name, 'model_weights')\n",
    "                self.actor_critic.save_weights(model_path)\n",
    "\n",
    "                # Save ob_rms from enviornment via pickle so that it can be restored when testing \n",
    "                vecnorm_path = os.path.join(self.model_dir, self.env_name, 'vecnorm_stats.pickle')\n",
    "                with open(vecnorm_path, 'wb') as handle:\n",
    "                    pickle.dump(getattr(self.env, 'ob_rms', None), handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        # Done with environment, can close\n",
    "        self.env.close()\n",
    "    \n",
    "    def test(self):\n",
    "        # Construct agent\n",
    "        self.construct_agent()\n",
    "\n",
    "        # Reconstruct environment, with NON-NORMALIZED REWARDS\n",
    "        self.env = DummyVecEnv([lambda: self.env]) # Needed to use the VecNormalize wrapper\n",
    "        self.env = VecNormalize(self.env, ob=True, ret=False)\n",
    "\n",
    "        # Restore model weights\n",
    "        model_path = os.path.join(self.model_dir, self.env_name, 'model_weights')\n",
    "        self.actor_critic.load_weights(model_path)\n",
    "\n",
    "        # Restored saved ob_rms to environment, disable updates to ob_rms\n",
    "        saved_ob_rms = None\n",
    "        vecnorm_path = os.path.join(self.model_dir, self.env_name, 'vecnorm_stats.pickle')\n",
    "        with open(vecnorm_path, 'rb') as handle:\n",
    "            saved_ob_rms = pickle.load(handle)\n",
    "\n",
    "        if saved_ob_rms is not None:\n",
    "            self.env.ob_rms = saved_ob_rms\n",
    "        self.env.eval()\n",
    "\n",
    "        state = self.env.reset() # Reset once at beginning, all subsequent resets handled by monitor\n",
    "\n",
    "        \"\"\" Only performing rollout for testing \"\"\"\n",
    "        while True: # Indefinitely performs rollout in simulator until closed\n",
    "            self.env.render() \n",
    "\n",
    "            _, action, _ = self.actor_critic.act(state)\n",
    "            next_state, _, _, _ = self.env.step(action)\n",
    "            \n",
    "            state = next_state\n",
    "\n",
    "        # Done with environment, can close\n",
    "        self.env.close()\n",
    "\n",
    "    def plot_results(self):\n",
    "        # Create plot directory\n",
    "        os.makedirs(self.plot_dir, exist_ok=True)\n",
    "\n",
    "        results = pu.load_results(os.path.join(self.log_dir, self.env_name.split('-')[0], ''))\n",
    "        pu.plot_results(results, average_group=True, split_fn=lambda _: '', shaded_std=False)\n",
    "        plt.xlabel('Timestep')\n",
    "        plt.ylabel('Reward')\n",
    "\n",
    "        fig = plt.gcf()\n",
    "        plot_path = os.path.join(self.plot_dir, 'plot_' + self.env_name)\n",
    "        fig.savefig(plot_path, bbox_inches='tight')\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "        \n",
    "\n",
    "env2 = gym.make('CartPole-v1')\n",
    "print(env2.observation_space)\n",
    "print(env.action_space)\n",
    "print(env2.action_space.n)\n",
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ba68eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alirahman/miniconda3/envs/tf/lib/python3.9/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment Reacher-v2 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 26\u001b[0m\n\u001b[1;32m      5\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNUM_ENV_TIMESTEPS\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m1e6\u001b[39m,\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNUM_TIMESTEPS_PER_UPDATE\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m2048\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSAVE_INTERVAL\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;241m25\u001b[39m\n\u001b[1;32m     23\u001b[0m }\n\u001b[1;32m     25\u001b[0m agent \u001b[38;5;241m=\u001b[39m PPO_Agent(params, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReacher-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m, model_dir, log_dir, plot_dir, seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 243\u001b[0m, in \u001b[0;36mPPO_Agent.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;241m=\u001b[39m VecNormalize(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, \n\u001b[1;32m    238\u001b[0m                         ob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ret\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \n\u001b[1;32m    239\u001b[0m                         gamma\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGAMMA\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;66;03m# Normalize states AND rewards for training\u001b[39;00m\n\u001b[1;32m    241\u001b[0m episode_reward_summary \u001b[38;5;241m=\u001b[39m deque(maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m--> 243\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Reset once at beginning, all subsequent resets handled by monitor\u001b[39;00m\n\u001b[1;32m    245\u001b[0m NUM_TRAIN_UPDATES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNUM_ENV_TIMESTEPS\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNUM_TIMESTEPS_PER_UPDATE\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    246\u001b[0m MINIBATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNUM_TIMESTEPS_PER_UPDATE\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNUM_MINIBATCHES\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[3], line 74\u001b[0m, in \u001b[0;36mVecNormalize.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mret \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs)\n\u001b[0;32m---> 74\u001b[0m     obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obfilt(obs)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/baselines/common/vec_env/dummy_vec_env.py:36\u001b[0m, in \u001b[0;36mDummyVecEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obs_tuple, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t,x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(obs_tuple):\n\u001b[0;32m---> 36\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuf_obs\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m[i] \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_obs[\u001b[38;5;241m0\u001b[39m][i] \u001b[38;5;241m=\u001b[39m obs_tuple\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "model_dir = \"models\"\n",
    "log_dir = \"logs\"\n",
    "plot_dir = \"plots\"\n",
    "\n",
    "params = {\n",
    "    'NUM_ENV_TIMESTEPS' : 1e6,\n",
    "    'NUM_TIMESTEPS_PER_UPDATE' : 2048,\n",
    "    'NUM_MINIBATCHES' : 32,\n",
    "    'NUM_EPOCHS' : 4,\n",
    "\n",
    "    'GAMMA' : 0.99,\n",
    "    'CLIP_PARAM' : 0.2,\n",
    "    'LAMBDA' : 0.95,\n",
    "    'VALUE_FUNCTION_COEF' : 0.5,\n",
    "    'ENTROPY_COEF' : 0,\n",
    "\n",
    "    'ACTOR_HIDDEN_UNITS' : [64, 64], \n",
    "    'CRITIC_HIDDEN_UNITS' : [64, 64],\n",
    "    'LEARNING_RATE' : 3e-4,\n",
    "    'OPTIMIZER_EPSILON' : 1e-7,\n",
    "\n",
    "    'SAVE_INTERVAL' : 25\n",
    "}\n",
    "\n",
    "agent = PPO_Agent(params, \"Reacher-v2\", model_dir, log_dir, plot_dir, seed=0)\n",
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0d8688",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
